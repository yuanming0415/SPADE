{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9bf2dd8-33cb-4e31-a845-6559594ff5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint were not used when initializing CLIPTextModel: \n",
      " ['text_model.embeddings.position_ids']\n",
      "Some weights of the model checkpoint were not used when initializing CLIPTextModelWithProjection: \n",
      " ['text_model.embeddings.position_ids']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RegionalDiffusionXLPipeline {\n",
       "  \"_class_name\": \"RegionalDiffusionXLPipeline\",\n",
       "  \"_diffusers_version\": \"0.27.2\",\n",
       "  \"feature_extractor\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"force_zeros_for_empty_prompt\": true,\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"EulerDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"text_encoder_2\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModelWithProjection\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"tokenizer_2\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RegionalDiffusion_base import RegionalDiffusionPipeline\n",
    "from RegionalDiffusion_xl import RegionalDiffusionXLPipeline\n",
    "from diffusers.schedulers import KarrasDiffusionSchedulers,DPMSolverMultistepScheduler\n",
    "from mllm import local_llm,GPT4\n",
    "import torch\n",
    "# If you want to load ckpt, initialize with \".from_single_file\".\n",
    "pipe = RegionalDiffusionXLPipeline.from_single_file(\"../mod/albedobaseXL_v20.safetensors\",torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
    "# If you want to use diffusers, initialize with \".from_pretrained\".\n",
    "# pipe = RegionalDiffusionXLPipeline.from_pretrained(\"path to your diffusers\",torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
    "pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf7581b1-acf6-4aec-a1f8-054045276cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config,use_karras_sigmas=True)\n",
    "pipe.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92e2a03-0be5-48bc-85d5-8a2045ee1bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yuanming test 1119 waiting for GPT-4 response\n"
     ]
    }
   ],
   "source": [
    "prompt= 'The black chair was on the left of the white table'\n",
    "para_dict = GPT4(prompt,key='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')\n",
    "## MLLM based split generation results\n",
    "split_ratio = para_dict['Final split ratio']\n",
    "regional_prompt = para_dict['Regional Prompt']\n",
    "negative_prompt = \"\" # negative_prompt, \n",
    "images = pipe(\n",
    "    prompt=regional_prompt,\n",
    "    split_ratio=split_ratio, # The ratio of the regional prompt, the number of prompts is the same as the number of regions\n",
    "    batch_size = 1, #batch size\n",
    "    base_ratio = 0.5, # The ratio of the base prompt    \n",
    "    base_prompt= prompt,       \n",
    "    num_inference_steps=20, # sampling step\n",
    "    height = 1024, \n",
    "    negative_prompt=negative_prompt, # negative prompt\n",
    "    width = 1024, \n",
    "    seed = 2024,# random seed\n",
    "    guidance_scale = 7.0\n",
    ").images[0]\n",
    "images.save(\"The black chair was on the left of the white table_000010.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e601dc-d785-4897-a958-af6e31dcf140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前显存使用:0.0 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"当前显存使用:{torch.cuda.memory_allocated() / (1024 ** 2)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf5e0631-820b-4324-a918-b9f5b67386b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20619f3-f563-412f-9833-7d5b8cc7c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RPGyuanming",
   "language": "python",
   "name": "rpgyuanming"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

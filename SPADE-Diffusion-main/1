import math
from pprint import pprint
import torch
import torchvision
import torchvision.transforms.functional as F
from torchvision.transforms import InterpolationMode, Resize 
import xformers
TOKENSCON = 77
TOKENS = 75

def generate_position_encoding(height, width, target_area=None, rsp=None, rep=None, csp=None, cep=None):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    position_encoding = torch.full((height, width), 1, device=device)
    rspf=rsp*height
    repf=rep*height
    cspf=csp*width
    cepf=cep*width
    rspf=int(rspf)
    repf=int(repf)
    cspf=int(cspf)
    cepf=int(cepf)
    if target_area == 'base':
        print("这里是在说Base哦")
        position_encoding[rspf:repf, cspf:cepf] = 1  # 左上
    else:
        if rsp == 0 and rep == 1.0:
        # 当 rsp=0 且 rep=1.0 时执行
            print("纵版")
            if csp == 0:
            # 当 rsp=0 且 rep=1.0 时执行
                cepf = 40
            elif cep == 1:
            # 当 csp=0 且 cep=1.0 时执行
                cspf = 60
        elif csp == 0 and cep == 1.0:
        # 当 csp=0 且 cep=1.0 时执行
            print("横版")
            if rsp == 0:
            # 当 rsp=0 且 rep=1.0 时执行
                repf = 40
            elif rep == 1:
            # 当 csp=0 且 cep=1.0 时执行
                rspf = 60
        print("这里在区域扩散了")
        position_encoding[rspf:repf, cspf:cepf] = 2  # 右下
        print("CCC")
        print(rspf)
        print(repf)
        print(cspf)
        print(cepf)
        print("CCC")
    return position_encoding

def _memory_efficient_attention_xformers(module, query, key, value, region_mask=None):
    query = query.contiguous()
    key = key.contiguous()
    value = value.contiguous()
    hidden_states = xformers.ops.memory_efficient_attention(query, key, value,attn_bias=None)
    hidden_states = module.batch_to_head_dim(hidden_states)
    return hidden_states

def main_forward_diffusers(module,hidden_states,encoder_hidden_states,divide,userpp = False,tokens=[],width = 100,height = 100,step = 0, isxl = False, inhr = None, target_area=None, rs=None, re=None, cs=None, ce=None):
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    position_encoding = generate_position_encoding(height, width, target_area, rsp = rs, rep = re, csp = cs, cep = ce).to(device)
    position_encoding_flat = position_encoding.view(-1)  # 转为一维
    tokens = hidden_states.size(1)  # tokens 数量，应该是 10000
    assert position_encoding_flat.size(0) == tokens
    position_encoding_expanded = position_encoding_flat.unsqueeze(0).expand(hidden_states.size(0), -1).unsqueeze(-1)
    context = encoder_hidden_states
    key = module.to_k(context)
    position_encoding_expanded = position_encoding_expanded.to(key.dtype)
    query = module.to_q(hidden_states) * position_encoding_expanded  # 加入位置编码
    value = module.to_v(context)
    query = module.head_to_batch_dim(query)
    key = module.head_to_batch_dim(key)
    value = module.head_to_batch_dim(value)
    hidden_states=_memory_efficient_attention_xformers(module, query, key, value)
    hidden_states = hidden_states.to(query.dtype)
    hidden_states = module.to_out[0](hidden_states)
    hidden_states = module.to_out[1](hidden_states)
    return hidden_states 
    
def hook_forwards(self, root_module: torch.nn.Module):
    for name, module in root_module.named_modules():
        if "attn2" in name and module.__class__.__name__ == "Attention":
            module.forward = hook_forward(self, module)           


def hook_forward(self, module):
    def forward(hidden_states, encoder_hidden_states=None, attention_mask=None, additional_tokens=None, n_times_crossframe_attn_in_self=0):
        x= hidden_states
        context= encoder_hidden_states
        height =self.h
        width =self.w
        x_t = x.size()[1]
        scale = round(math.sqrt(height * width / x_t))
        latent_h = round(height / scale)
        latent_w = round(width / scale)
        ha, wa = x_t % latent_h, x_t % latent_w
        if ha == 0:
            latent_w = int(x_t / latent_h)
        elif wa == 0:
            latent_h = int(x_t / latent_w)
        contexts = context.clone()
        def matsepcalc(x,contexts,pn,divide):
            h_states = []
            x_t = x.size()[1]
            (latent_h,latent_w) = split_dims(x_t, height, width, self)
            latent_out = latent_w
            latent_in = latent_h
            tll = self.pt
            i = 0
            outb = None
            if self.usebase:
                context = contexts[:,tll[i][0] * TOKENSCON:tll[i][1] * TOKENSCON,:]
                cnet_ext = contexts.shape[1] - (contexts.shape[1] // TOKENSCON) * TOKENSCON
                if cnet_ext > 0:
                    context = torch.cat([context,contexts[:,-cnet_ext:,:]],dim = 1)
                i = i + 1
                if x.shape[1] == 10000:
                    out = main_forward_diffusers(module, x, context, divide,userpp =True, width =100, height =100, isxl = self.isxl, target_area = "base", rs = 0, re = 1, cs = 0, ce = 1)
                elif x.shape[1] == 2500:
                    out = main_forward_diffusers(module, x, context, divide,userpp =True, width =50, height =50, isxl = self.isxl, target_area = "base", rs = 0, re = 1, cs = 0, ce = 1)
                outb = out.clone()
                outb = outb.reshape(outb.size()[0], latent_h, latent_w, outb.size()[2]) 
            sumout = 0
            for drow in self.split_ratio:
                hstart = drow.start
                hend = drow.end
                v_states = []
                sumin = 0
                for dcell in drow.cols:
                    wstart=dcell.start
                    wend=dcell.end
                    context = contexts[:,tll[i][0] * TOKENSCON:tll[i][1] * TOKENSCON,:]
                    cnet_ext = contexts.shape[1] - (contexts.shape[1] // TOKENSCON) * TOKENSCON
                    if cnet_ext > 0:
                        context = torch.cat([context,contexts[:,-cnet_ext:,:]],dim = 1)
                    i = i + 1 + dcell.breaks
                    if x.shape[1] == 10000:
                        out = main_forward_diffusers(module, x, context, divide,userpp =True, width =100, height =100, isxl = self.isxl, target_area = "bottom_right", rs = hstart, re = hend, cs = wstart, ce = wend)
                    elif x.shape[1] == 2500:
                        out = main_forward_diffusers(module, x, context, divide,userpp =True, width =50, height =50, isxl = self.isxl, target_area = "bottom_right", rs = hstart, re = hend, cs = wstart, ce = wend)
                    out = out.reshape(out.size()[0], latent_h, latent_w, out.size()[2])
                    addout = 0
                    addin = 0
                    sumin = sumin + int(latent_in*dcell.end) - int(latent_in*dcell.start)
                    if dcell.end >= 0.999:
                        addin = sumin - latent_in
                        sumout = sumout + int(latent_out*drow.end) - int(latent_out*drow.start)
                        if drow.end >= 0.999:
                            addout = sumout - latent_out
                    out = out[:,int(latent_h*drow.start) + addout:int(latent_h*drow.end),
                                int(latent_w*dcell.start) + addin:int(latent_w*dcell.end),:]
                    if self.usebase : 
                        outb_t = outb[:,int(latent_h*drow.start) + addout:int(latent_h*drow.end),
                                        int(latent_w*dcell.start) + addin:int(latent_w*dcell.end),:].clone()
                        out = out * (1 - dcell.base) + outb_t * dcell.base
                    v_states.append(out)
                output_x = torch.cat(v_states,dim = 2)
                h_states.append(output_x)
            output_x = torch.cat(h_states,dim = 1) 
            output_x = output_x.reshape(x.size()[0],x.size()[1],x.size()[2])
            return output_x
        if x.size()[0] == 1 * self.batch_size:
            output_x = matsepcalc(x, contexts, self.pn, 1)
        else:
            if self.isvanilla:
                nx, px = x.chunk(2)
                conn,conp = contexts.chunk(2)
            else:
                px, nx = x.chunk(2)
                conp,conn = contexts.chunk(2)
            opx = matsepcalc(px, conp, True, 2)
            onx = matsepcalc(nx, conn, False, 2)
            if self.isvanilla:
                output_x = torch.cat([onx, opx])
            else:
                output_x = torch.cat([opx, onx]) 
        self.pn = not self.pn
        self.count = 0
        return output_x

    return forward

def split_dims(x_t, height, width, self=None):
    scale = math.ceil(math.log2(math.sqrt(height * width / x_t)))
    latent_h = repeat_div(height, scale)
    latent_w = repeat_div(width, scale)
    if x_t > latent_h * latent_w and hasattr(self, "nei_multi"):
        latent_h, latent_w = self.nei_multi[1], self.nei_multi[0] 
        while latent_h * latent_w != x_t:
            latent_h, latent_w = latent_h // 2, latent_w // 2

    return latent_h, latent_w

def repeat_div(x,y):
    while y > 0:
        x = math.ceil(x / 2)
        y = y - 1
    return x
